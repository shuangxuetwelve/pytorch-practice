{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import os\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# Third-party imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Own imports\n",
    "import utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH, HEIGHT = 28, 28\n",
    "CATEGORIES = 10\n",
    "BATCH_SIZE = 10\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 10\n",
    "KERNEL_SIZES = [5, 5]\n",
    "CHANNELS = [1, 6, 16]\n",
    "FC_LAYER_SIZES = [120, 84]\n",
    "SAVED_FILENAME = 'MNIST-CNN-LeNet'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets.\n",
    "dataset_training = torchvision.datasets.MNIST('./data', download=True, train=True, transform=torchvision.transforms.ToTensor())\n",
    "dataset_test = torchvision.datasets.MNIST('./data', download=True, train=False, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "# Create data loaders.\n",
    "dataloader_training = torch.utils.data.DataLoader(dataset_training, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
      "  (1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (4): Flatten(start_dim=1, end_dim=-1)\n",
      "  (5): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (6): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (7): Linear(in_features=84, out_features=10, bias=True)\n",
      "  (8): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_inputs_to_fc_layer = int(((WIDTH / 2 - 4) / 2) * ((HEIGHT / 2 - 4) / 2)) * CHANNELS[2]\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(CHANNELS[0], CHANNELS[1], KERNEL_SIZES[0], padding='same'),\n",
    "    torch.nn.AvgPool2d(2),\n",
    "    torch.nn.Conv2d(CHANNELS[1], CHANNELS[2], KERNEL_SIZES[1]),\n",
    "    torch.nn.AvgPool2d(2),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(num_inputs_to_fc_layer, FC_LAYER_SIZES[0]),\n",
    "    torch.nn.Linear(FC_LAYER_SIZES[0], FC_LAYER_SIZES[1]),\n",
    "    torch.nn.Linear(FC_LAYER_SIZES[1], CATEGORIES),\n",
    "    torch.nn.Softmax(dim=1),\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Statistics of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 61706\n"
     ]
    }
   ],
   "source": [
    "num_parameters_total = 0\n",
    "for params in model.parameters():\n",
    "    num_parameters = 1\n",
    "    size = params.size()\n",
    "    for dim_size in size:\n",
    "        num_parameters *= dim_size\n",
    "    num_parameters_total += num_parameters\n",
    "\n",
    "print(f'Total number of parameters: {num_parameters_total}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateStatistics(dataloader, model, loss_fn):\n",
    "    with torch.no_grad():\n",
    "        num_batches = len(dataloader)\n",
    "        num_samples = len(dataloader.dataset)\n",
    "        loss, correct = 0, 0\n",
    "\n",
    "        for images, labels in dataloader:\n",
    "            # Make predictions.\n",
    "            preds = model(images)\n",
    "\n",
    "            # Calculate the loss.\n",
    "            loss += loss_fn(preds, labels).item()\n",
    "\n",
    "            # Calculate the number of correctnesses.\n",
    "            correct += (preds.argmax(dim=1) == labels).type(dtype=torch.float32).sum().item()\n",
    "\n",
    "        # Calculate the mean loss and correct.\n",
    "        loss /= num_batches\n",
    "        correct /= num_samples\n",
    "\n",
    "    return loss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGrads(model):\n",
    "    grads_weight = []\n",
    "    grads_bias = []\n",
    "\n",
    "    for layer in model:\n",
    "        if type(layer) == torch.nn.Conv2d or type(layer) == torch.nn.Linear:\n",
    "            grads_weight.append(layer.weight.grad.clone())\n",
    "            grads_bias.append(layer.bias.grad.clone())\n",
    "\n",
    "    return grads_weight, grads_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(model):\n",
    "    # Retrieve saved data.\n",
    "    data = torch.load(f'saved/{SAVED_FILENAME}.pt')\n",
    "\n",
    "    # Resume model from saved data.\n",
    "    state_dict = data['state_dict']\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    # Return losses and accuracies for training and testing.\n",
    "    return data['losses'], data['corrects'], data['losses_test'], data['corrects_test'], data['grads'], data['duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, losses, corrects, losses_test, corrects_test, grads, duration):\n",
    "    # Create folder 'saved' if it does not exist.\n",
    "    if not os.path.exists('saved'):\n",
    "        os.makedirs('saved')\n",
    "\n",
    "    # Construct the struct of data to save.\n",
    "    data = {\n",
    "        'state_dict': model.state_dict(),\n",
    "        'losses': losses,\n",
    "        'corrects': corrects,\n",
    "        'losses_test': losses_test,\n",
    "        'corrects_test': corrects_test,\n",
    "        'grads': grads,\n",
    "        'duration': duration,\n",
    "    }\n",
    "\n",
    "    # Save data to the file.\n",
    "    torch.save(data, f'saved/{SAVED_FILENAME}.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, learning_rate):\n",
    "    # Define a timer and start timing.\n",
    "    t = utils.Timer()\n",
    "    t.start()\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), learning_rate)\n",
    "\n",
    "    # Define losses and corrects for training and test.\n",
    "    losses, corrects = [], []\n",
    "    losses_test, corrects_test = [], []\n",
    "\n",
    "    # Define the grad variables.\n",
    "    grads = []\n",
    "\n",
    "    # Calculate losses and corrects before training.\n",
    "    loss, correct = calculateStatistics(dataloader_training, model, loss_fn)\n",
    "    losses.append(loss)\n",
    "    corrects.append(correct)\n",
    "    loss_test, correct_test = calculateStatistics(dataloader_test, model, loss_fn)\n",
    "    losses_test.append(loss)\n",
    "    corrects_test.append(correct_test)\n",
    "\n",
    "    # Print statistics.\n",
    "    print(f'Training error: loss: {loss:>7f}, accuracy: {correct*100:>0.1f}%')\n",
    "    print(f'Test error: loss: {loss_test:>7f}, accuracy: {correct_test*100:>0.1f}%')\n",
    "\n",
    "    # Run epochs.\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Run one epoch for training.\n",
    "        num_batches = len(dataloader_training)\n",
    "        num_samples = len(dataloader_training.dataset)\n",
    "        loss_epoch, correct_epoch = 0, 0\n",
    "        grads_weight_epoch, grads_bias_epoch = None, None\n",
    "        for batch, [images, labels] in enumerate(dataloader_training):\n",
    "            # Make predictions.\n",
    "            pred = model(images)\n",
    "\n",
    "            # Calculate the loss.\n",
    "            loss = loss_fn(pred, labels)\n",
    "            loss_epoch += loss.item()\n",
    "\n",
    "            # Calculate the number of correctness.\n",
    "            correct_epoch += (pred.argmax(dim=1) == labels).type(dtype=torch.float32).sum().item()\n",
    "\n",
    "            # Calculate gradients.\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Store the gradients.\n",
    "            grads_weight_batch, grads_bias_batch = getGrads(model)\n",
    "            if grads_weight_epoch == None:\n",
    "                grads_weight_epoch = grads_weight_batch\n",
    "            else:\n",
    "                for i in range(len(grads_weight_epoch)):\n",
    "                    grads_weight_epoch[i] += grads_weight_batch[i]\n",
    "            if  grads_bias_epoch == None:\n",
    "                grads_bias_epoch = grads_bias_batch\n",
    "            else:\n",
    "                for i in range(len(grads_bias_epoch)):\n",
    "                    grads_bias_epoch[i] += grads_bias_batch[i]\n",
    "\n",
    "            # Optimization.\n",
    "            optimizer.step()\n",
    "\n",
    "            if (batch + 1) % 1000 == 0:\n",
    "                loss, size_current, size_total = loss.item(), (batch+1)*BATCH_SIZE, len(dataloader_training.dataset)\n",
    "                print(f'loss: {loss:>7f} [{size_current:>5d} / {size_total:>5d}]')\n",
    "            \n",
    "        # Calculate the mean loss across all batches.\n",
    "        loss_epoch /= num_batches\n",
    "        losses.append(loss_epoch)\n",
    "\n",
    "        # Calculate the mean number of correctness across all batches.\n",
    "        correct_epoch /= num_samples\n",
    "        corrects.append(correct_epoch)\n",
    "\n",
    "        for i in range(len(grads_bias_epoch)):\n",
    "            grads_weight_epoch[i] /= num_batches\n",
    "        for i in range(len(grads_bias_epoch)):\n",
    "            grads_bias_epoch[i] /= num_batches\n",
    "        grads.append({\n",
    "            'weight': grads_weight_epoch,\n",
    "            'bias': grads_bias_epoch,\n",
    "        })\n",
    "\n",
    "        # Print statistics of the training part of the epoch.\n",
    "        print(f'Training error: loss: {loss_epoch:>7f}, accuracy: {correct_epoch*100:>0.1f}%')\n",
    "\n",
    "        # Calculate loss and correct for test.\n",
    "        loss_test, correct_test = calculateStatistics(dataloader_test, model, loss_fn)\n",
    "        losses_test.append(loss_test)\n",
    "        corrects_test.append(correct_test)\n",
    "\n",
    "        # Print statistics of the test part of the epoch.\n",
    "        print(f'Test error: loss: {loss_test:>7f}, accuracy: {correct_test*100:>0.1f}%')\n",
    "\n",
    "    # Stop timing.\n",
    "    t.stop()\n",
    "\n",
    "    # Return losses and accuracies for training and testing, as well as the training duration.\n",
    "    return losses, corrects, losses_test, corrects_test, grads, t.getDuration()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: loss: 2.302813, accuracy: 11.6%\n",
      "Test error: loss: 2.302734, accuracy: 12.2%\n",
      "loss: 2.295628 [10000 / 60000]\n",
      "loss: 2.302418 [20000 / 60000]\n",
      "loss: 2.294107 [30000 / 60000]\n",
      "loss: 2.268204 [40000 / 60000]\n",
      "loss: 1.771669 [50000 / 60000]\n",
      "loss: 1.671700 [60000 / 60000]\n",
      "Training error: loss: 2.163523, accuracy: 35.1%\n",
      "Test error: loss: 1.683829, accuracy: 78.9%\n",
      "loss: 1.671977 [10000 / 60000]\n",
      "loss: 1.464268 [20000 / 60000]\n",
      "loss: 1.660773 [30000 / 60000]\n",
      "loss: 1.661551 [40000 / 60000]\n",
      "loss: 1.657370 [50000 / 60000]\n",
      "loss: 1.856379 [60000 / 60000]\n",
      "Training error: loss: 1.653577, accuracy: 81.2%\n",
      "Test error: loss: 1.630820, accuracy: 83.2%\n",
      "loss: 1.661657 [10000 / 60000]\n",
      "loss: 1.564000 [20000 / 60000]\n",
      "loss: 1.559893 [30000 / 60000]\n",
      "loss: 1.559929 [40000 / 60000]\n",
      "loss: 1.567895 [50000 / 60000]\n",
      "loss: 1.530945 [60000 / 60000]\n",
      "Training error: loss: 1.590890, accuracy: 87.3%\n",
      "Test error: loss: 1.568101, accuracy: 89.4%\n",
      "loss: 1.541304 [10000 / 60000]\n",
      "loss: 1.731233 [20000 / 60000]\n",
      "loss: 1.758389 [30000 / 60000]\n",
      "loss: 1.564464 [40000 / 60000]\n",
      "loss: 1.562413 [50000 / 60000]\n",
      "loss: 1.465513 [60000 / 60000]\n",
      "Training error: loss: 1.573620, accuracy: 88.9%\n",
      "Test error: loss: 1.560606, accuracy: 90.2%\n",
      "loss: 1.665915 [10000 / 60000]\n",
      "loss: 1.560934 [20000 / 60000]\n",
      "loss: 1.461272 [30000 / 60000]\n",
      "loss: 1.596977 [40000 / 60000]\n",
      "loss: 1.572875 [50000 / 60000]\n",
      "loss: 1.658873 [60000 / 60000]\n",
      "Training error: loss: 1.569345, accuracy: 89.3%\n",
      "Test error: loss: 1.557885, accuracy: 90.3%\n",
      "loss: 1.462740 [10000 / 60000]\n",
      "loss: 1.627203 [20000 / 60000]\n",
      "loss: 1.483130 [30000 / 60000]\n",
      "loss: 1.565859 [40000 / 60000]\n",
      "loss: 1.461160 [50000 / 60000]\n",
      "loss: 1.586803 [60000 / 60000]\n",
      "Training error: loss: 1.567214, accuracy: 89.4%\n",
      "Test error: loss: 1.560801, accuracy: 90.1%\n",
      "loss: 1.661007 [10000 / 60000]\n",
      "loss: 1.657075 [20000 / 60000]\n",
      "loss: 1.740330 [30000 / 60000]\n",
      "loss: 1.461891 [40000 / 60000]\n",
      "loss: 1.761731 [50000 / 60000]\n",
      "loss: 1.544005 [60000 / 60000]\n",
      "Training error: loss: 1.564068, accuracy: 89.7%\n",
      "Test error: loss: 1.558611, accuracy: 90.2%\n",
      "loss: 1.461471 [10000 / 60000]\n",
      "loss: 1.562877 [20000 / 60000]\n",
      "loss: 1.536589 [30000 / 60000]\n",
      "loss: 1.636559 [40000 / 60000]\n",
      "loss: 1.551659 [50000 / 60000]\n",
      "loss: 1.570777 [60000 / 60000]\n",
      "Training error: loss: 1.562453, accuracy: 89.9%\n",
      "Test error: loss: 1.552418, accuracy: 90.9%\n",
      "loss: 1.584734 [10000 / 60000]\n",
      "loss: 1.522794 [20000 / 60000]\n",
      "loss: 1.558118 [30000 / 60000]\n",
      "loss: 1.641988 [40000 / 60000]\n",
      "loss: 1.560720 [50000 / 60000]\n",
      "loss: 1.462327 [60000 / 60000]\n",
      "Training error: loss: 1.562078, accuracy: 89.9%\n",
      "Test error: loss: 1.558059, accuracy: 90.4%\n",
      "loss: 1.561338 [10000 / 60000]\n",
      "loss: 1.461153 [20000 / 60000]\n",
      "loss: 1.559054 [30000 / 60000]\n",
      "loss: 1.537608 [40000 / 60000]\n",
      "loss: 1.461155 [50000 / 60000]\n",
      "loss: 1.461487 [60000 / 60000]\n",
      "Training error: loss: 1.560646, accuracy: 90.0%\n",
      "Test error: loss: 1.552793, accuracy: 90.9%\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(f'saved/{SAVED_FILENAME}.pt'):\n",
    "    losses, corrects, losses_test, corrects_test, grads, duration = load(model)\n",
    "else:\n",
    "    losses, corrects, losses_test, corrects_test, grads, duration = train(model, EPOCHS, LEARNING_RATE)\n",
    "    save(model, losses, corrects, losses_test, corrects_test, grads, duration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
